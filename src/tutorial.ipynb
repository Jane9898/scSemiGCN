{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed explanation of running scSemiGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enhancement\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from TwoLayerGCN import GCN\n",
    "import argparse\n",
    "from validation import eval, make_prediction\n",
    "import torch.nn as nn\n",
    "import enhancement\n",
    "import random\n",
    "import os\n",
    "from PseudoLabels import knn_similarity\n",
    "from contrastive_loss import contrastive_loss\n",
    "from OneLayerGCN import preprocessor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Set random set for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1029):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed) \n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions for training GCNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(args, model, feature, pseudo_labels, loss_func, optimizer):\n",
    "    feature = feature.float()\n",
    "    for i in range(args.round):\n",
    "        feature_ = model(feature)\n",
    "        loss = loss_func(feature_, pseudo_labels, args.tau)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('contrastive loss[%d/%d]: %.4f' % (i+1, args.round, loss))\n",
    "    \n",
    "    feature_ = model(feature)\n",
    "    \n",
    "    return feature_\n",
    "\n",
    "\n",
    "def train(model, features, loss, optimizer, train_dataset, num_class, epochs):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        losses_train = 0\n",
    "        for x in train_dataset:\n",
    "            pro = model(features)\n",
    "            y = x.T[0]-1\n",
    "            a = pro[x.T[1].long()]\n",
    "            loss_ = loss(a, y.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss_.backward()\n",
    "            optimizer.step()\n",
    "            losses_train += loss_.cpu().item()\n",
    "\n",
    "        eval_train = eval(pro, train_dataset, num_class)\n",
    "       \n",
    "        print(\"In epoch: %d, losses: %.4f, acc_train:%.4f, f1_train:%.4f, ,auc_train:%.4f\" \n",
    "              % (i+1, losses_train, eval_train[0],  eval_train[1], eval_train[2]))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter settings\n",
    "Set parameters for running scSemiGCN. Descriptions of paramaters can be found in the README file. Please change the default values to set your own settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # parameters for topological denoising\n",
    "        self.Nk = 18\n",
    "        self.alpha = 0.5\n",
    "\n",
    "        # parameters for feature refinement\n",
    "        self.round = 10\n",
    "        self.dropout = 0.5\n",
    "        self.slr = 0.05\n",
    "        self.weight_decay = 1e-2\n",
    "        self.tau = 0.5\n",
    "\n",
    "        # paramters for semi-supervised cell-type annotation\n",
    "        self.hidden = 100\n",
    "        self.glr = 0.002\n",
    "        self.epoch = 100\n",
    "        self.batch_size = 100\n",
    "        self.dir = \"Prediction\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters and load data in a specified format\n",
    "The file seqdata.mat includes three fields. The format is explained in the README file. Gold-standard annotation of labeled cells is assigned to the variable *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch()\n",
    "args = Params()\n",
    "data = loadmat(\"seqdata.mat\")\n",
    "\n",
    "adj = data[\"similarity\"]\n",
    "annotation = data[\"annotation\"].ravel()\n",
    "feature = data[\"feature\"]\n",
    "\n",
    "break_idx = np.where(annotation==-1)[0][0]\n",
    "labels = annotation[:break_idx]\n",
    "    \n",
    "feat_dim = feature.shape[1]\n",
    "num_class = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate pseudo labels with topoloigcal denoising\n",
    "We frist obtain a denoised similarity matrix by network enhancement, then generate pseudo labels for unannotated cells by 1-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = enhancement.network_enhancement(adj, 2, 18, 0.5) \n",
    "pseudo_labels = knn_similarity(labels, adj, 1)\n",
    "\n",
    "adj = torch.from_numpy(adj)\n",
    "adj = adj.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine features with supervised contrastive learning in a denoised GCN.\n",
    "We define a denoised GCN *premodel* with the denoised network structure *adj* and obtain refined features *refined_feature* with supervised contrastive learning using pseudo labels *pseudo_labels*. The *contrastive_loss* is the corresponding supervised contrastive loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = torch.from_numpy(feature)\n",
    "pseudo_labels = torch.tensor(pseudo_labels, dtype=torch.int32)\n",
    "    \n",
    "premodel = preprocessor(feat_dim, args.dropout, adj)\n",
    "optimizer1 = torch.optim.SGD(params=premodel.parameters(), lr=args.slr, weight_decay=args.weight_decay)\n",
    "refined_feature = pretrain(args, premodel, feature, pseudo_labels, contrastive_loss, optimizer1).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a cell-type annotation GCN with gold-standard annotated cells\n",
    "A two-layer denoised GCN *net* is trained by gold-standard annotated cells with refine features, using cross entropy as loss function. The well-trained model is returned as *opt_model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.arange(break_idx)\n",
    "train_data = np.vstack((labels, train_idx))\n",
    "train_data = torch.tensor(train_data.T, dtype=torch.float)\n",
    "train_dataset = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "net = GCN(feat_dim, args.hidden, num_class, args.dropout, adj)\n",
    "optimizer2 = torch.optim.Adam(params=net.parameters(), lr=args.glr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "opt_model = train(net, refined_feature, loss, optimizer2, train_dataset, num_class, args.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate unlabeld cells by *opt_model*. \n",
    "The results are stored in the table *prediction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = opt_model(refined_feature)\n",
    "unannotated_idx = np.arange(len(labels), len(annotation))\n",
    "to_anno_data = torch.tensor(unannotated_idx, dtype=torch.float)\n",
    "to_anno_dataset = DataLoader(to_anno_data, batch_size=args.batch_size, shuffle=False)\n",
    "prediction_ = make_prediction(pro, to_anno_dataset)\n",
    "prediction = pd.DataFrame(prediction_, columns=[\"sampleID\", \"predictedLabel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results\n",
    "Save annotation and the well-trained model to a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.dir):\n",
    "     os.makedirs(args.dir)\n",
    "prediction.to_csv(args.dir + \"/\" + \"make_prediction.csv\", index=False)\n",
    "torch.save(opt_model.state_dict(), args.dir + \"/\" + \"opt_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
